<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions</title>
  <style>
    /* 段落文字的样式 */
    .paragraph {
      font-size: 1.5em; /* 设置较大的字体大小 */
      margin-top: 0.5em; /* 可以根据需要调整段落间的间距 */
      text-align: left; /* 左对齐文本 */
    }

    /* 前导点的样式 */
    .dot {
      margin-right: 0.5em; /* 调整前导点与文字之间的距离 */
      font-size: inherit; /* 继承文字的字体大小 */
      line-height: inherit; /* 继承文字的行高 */
    }
  </style>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y5ZVQZ7NHC"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js_slide/fontawesome.all.min.js"></script>
  <script src="./static/js_slide/bulma-carousel.min.js"></script>
  <script src="./static/js_slide/bulma-slider.min.js"></script>
  <script src="./static/js_slide/index.js"></script>


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Vendor Stylesheets -->
  <!--=================js==========================-->
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/juxtapose.css">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="font-weight: bold; font-style: italic">F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yangjie-cv.github.io/">Jie Yang</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://nxsedson.github.io/">Xuesong Niu</a><sup>2*</sup>,</span>
            <span class="author-block">
                            <a href="https://scholar.google.com/citations?user=xSYWn08AAAAJ&hl=zh-CN">Nan Jiang</a><sup>2,3*</sup>,</span>
            <span class="author-block">
              <a href="http://www.zhangruimao.site/">Ruimao Zhang</a><sup>1†</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://siyuanhuang.com/">Siyuan Huang</a><sup>2†</sup></span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Chinese University of Hong Kong, Shenzhen <br>
              <sup>2</sup>Beijing Institute for General Artificial Intelligence (BIGAI),
              <sup>3</sup>Institute for AI, Peking University<br>
              *equal contribution †corresponding author
          </div>
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.07221" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                              <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                            
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!--=================Overview of UniPose==========================-->
<section class="section" >
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Semantic-HOI</h2>
        <!-- <h2 class="subtitle has-text-centered">We use eight metrics covering four aspects: structure distance, background preservation, edit prompt-image consistency, and inference time.</h2> -->
        <div class="content has-text-justified">
          <div style="text-align: center; vertical-align:middle">
            <img src="f-hoi/figures/semantic_hoi.jpg" width="600">
          </div>

          <!-- 第一句话，已加粗并且字体更大 -->
          <p style="font-size: 1.2em; font-weight: bold; font-style: italic">
           </span> we introduce a
new dataset named Semantic-HOI to bridge the annotation gap present in current
datasets by providing fine-grained descriptions for each HOI state and the body
movement between two consecutive states.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview: A Generalist Keypoint Detector</h2>

        <!-- 第一句话，已加粗 -->
        <p class="paragraph"><span class="dot">&#8226;</span> <span style="font-weight: bold;">UniPose is the first end-to-end prompt-based keypoint detection framework.</span></p>

        <!-- 第二句话，已加粗 -->
        <p class="paragraph"><span class="dot">&#8226;</span> <span style="font-weight: bold;">UniPose could support visual or textual prompts for any articulated, rigid, and soft objects.</span></p>

        <!-- 第三句话，已加粗 -->
        <p class="paragraph"><span class="dot">&#8226;</span> <span style="font-weight: bold;">UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses.</span></p>
      
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="section-title has-text-centered" >
    <h2 class="title is-3 is-centered">Visual Prompt as Input</h2>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="f-hoi/figures/visual_prompt.jpg" style=" width: 2048px;">
    </div>
  </div>
  <div class="section-title has-text-centered" >
    <h2 class="title is-3 is-centered">Textual Prompt as Input</h2>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="f-hoi/figures/texual_prompt.jpg" style=" width: 2048px;">
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="section-title has-text-centered" >
    <h2 class="title is-3 is-centered">Test on arbitrary in-the-wild images</h2>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="f-hoi/figures/test.png" style=" width: 2048px;">
    </div>
  </div>
  <div class="section-title has-text-centered" >
    <h2 class="title is-3 is-centered">Test on existing datasets</h2>
  </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="f-hoi/figures/test2.png" style=" width: 2048px;">
    </div>
  </div>
</section>








<!-- <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="content has-text-justified">
          <div style="text-align: center; vertical-align:middle">
            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./demo_video_v5.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div align="center">
            <b>Samples generated by ModelScope with or without FreeU.</b>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!--=================Abstract==========================-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This work proposes a unified framework called UniPose to detect keypoints of
            any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation. Keypoint
            is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects. Existing fine-grained promptable tasks mainly focus on
            object instance detection and segmentation but often fail to identify fine-grained
            granularity and structured information of image and instance, such as eyes, leg,
            paw, etc. Meanwhile, prompt-based keypoint detection is still under-explored. To
            bridge the gap, we make the first attempt to develop an end-to-end prompt-based
            keypoint detection framework called UniPose to detect keypoints of any objects.
            As keypoint detection tasks are unified in this framework, we can leverage 13
            keypoint detection datasets with 338 keypoints across 1,237 categories over 400K
            instances to train a generic keypoint detection model. UniPose can effectively
            align text-to-keypoint and image-to-keypoint due to the mutual enhancement of
            textual and visual prompts based on the cross-modality contrastive learning optimization objectives. Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories,
            and poses. Based on UniPose as a generalist keypoint detector, we hope it could
            serve fine-grained visual perception, understanding, and generation.
          </p>
          </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>







<!--=================Overview of UniPose==========================-->
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework of UniPose </h2>
        <!-- <h2 class="subtitle has-text-centered">We use eight metrics covering four aspects: structure distance, background preservation, edit prompt-image consistency, and inference time.</h2> -->
        <div class="content has-text-justified">
          <div style="text-align: center; vertical-align:middle">
            <img src="f-hoi/figures/framework.png" width="1024">
          </div>

          <!-- 修改后的文本，字体大小增大 -->
          <p style="font-size: 1.5em;">
            <span style="font-weight: bold; font-style: italic">Highlight: UniPose can effectively
           align text-to-keypoint and image-to-keypoint due to the mutual enhancement of
           textual and visual prompts based on the cross-modality contrastive learning optimization objectives.</span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>






<!--=================Overview of UniPose==========================-->
<section class="section" >
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Unifying 13 datasets into the UniKPT dataset for effective training</h2>
        <!-- <h2 class="subtitle has-text-centered">We use eight metrics covering four aspects: structure distance, background preservation, edit prompt-image consistency, and inference time.</h2> -->
        <div class="content has-text-justified">
          <div style="text-align: center; vertical-align:middle">
            <img src="f-hoi/figures/dataset.png" width="1024">
            <img src="f-hoi/figures/dataset2.png" width="1024">
          </div>
    </div>
  </div>
</section>




<section class="section" >
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contact Us!</h2>
        <!-- <h2 class="subtitle has-text-centered">We use eight metrics covering four aspects: structure distance, background preservation, edit prompt-image consistency, and inference time.</h2> -->
        <div class="content has-text-justified">
          <p style="font-size: 1.2em; font-weight: bold;">
            For detailed questions about this work, please contact jieyang5@link.cuhk.edu.cn;
            We are looking for talented, motivated, and creative research and engineering interns working on human-centric visual understanding and generation topics. If you are interested, please send your CV to Ailing Zeng (zengailing@idea.edu.cn).
             
          </p>
    </div>
  </div>
</section>




  
<section class="section" >
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Cite Us!</h2>
        <!-- <h2 class="subtitle has-text-centered">We use eight metrics covering four aspects: structure distance, background preservation, edit prompt-image consistency, and inference time.</h2> -->
        <div class="content has-text-justified">
          <pre><code>@article{yang2023unipose,
            title={UniPose: Detection Any Keypoints},
            author={Yang, Jie and Zeng, Ailing and Zhang, Ruimao and Zhang, Lei},
            journal={arXiv preprint arXiv:2310.08530},
            year={2023}
          }
      </code></pre>
      <pre><code>@inproceedings{yang2022explicit,
        title={Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation},
        author={Yang, Jie and Zeng, Ailing and Liu, Shilong and Li, Feng and Zhang, Ruimao and Zhang, Lei},
        booktitle={The Eleventh International Conference on Learning Representations},
        year={2022}
      }
      </code></pre>
      <pre><code>@inproceedings{yang2023neural,
        title={Neural Interactive Keypoint Detection},
        author={Yang, Jie and Zeng, Ailing and Li, Feng and Liu, Shilong and Zhang, Ruimao and Zhang, Lei},
        booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
        pages={15122--15132},
        year={2023}
      }
      </code></pre>
    </div>
  </div>
</section>







  



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
<!--       <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
<!--       <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
